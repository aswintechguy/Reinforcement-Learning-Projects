{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlXftOLAKW_4"
   },
   "source": [
    "## RL Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4PPdGubXmKG9"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "!pip install gym\n",
    "!pip install nqueens\n",
    "!pip install -U ray\n",
    "\n",
    "!gdown --id 1lXBaqqOzMnmr7F04T9H2UnbTyN9g4Pjs\n",
    "!gdown --id 1TTByOPj9JvBGS6I5B71u5qxaKKIXB9JZ\n",
    "!gdown --id 1SPeH3NlXETsdVzRTV0YqwWRwcRmUMgao\n",
    "\n",
    "!mkdir /content/save\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1bTDB4hEmPSr"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, MultiDiscrete\n",
    "from nqueens import Queen\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import tqdm\n",
    "import os\n",
    "import gc\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "import cv2\n",
    "# from google.colab.patches import cv2_imshow\n",
    "\n",
    "\n",
    "from IPython.display import HTML\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from base64 import b64encode\n",
    "\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fABA6t4wmRTV"
   },
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qq27G6JDmQbQ"
   },
   "outputs": [],
   "source": [
    "class Vasuki(Env):\n",
    "\n",
    "    def _food_position_(self, n):\n",
    "        # Using the N-Queens problem to uniformly distribute the food spawning location\n",
    "        qq = Queen(n)\n",
    "        food_pos = np.empty(shape = [0, 2])\n",
    "        chess = qq.queen_data[0]\n",
    "        for x in range(n):\n",
    "            for y in range(n):\n",
    "                if chess[y][x] == 1:\n",
    "                    arr = np.array([[x, y]])\n",
    "                    food_pos = np.append(food_pos, arr, axis = 0)\n",
    "        # Returning the n food locations which are spatially distributed uniformly\n",
    "        return food_pos\n",
    "    \n",
    "    def _init_agent_(self, score=0):\n",
    "        # Creating a dictionary to store the information related to the agent\n",
    "        agent = {}\n",
    "        # Set initial direction of head of the Snake :  North = 0, East = 1, South = 2, West = 3\n",
    "        agent['head'] = np.random.randint(low = 0, high = 4, size = (1)).item()\n",
    "        # The score for each agent\n",
    "        agent['score'] = score\n",
    "        # Set initial position \n",
    "        agent['state'] = np.random.randint(low = 0, high = self.n, size = (2))\n",
    "        # Velocity of the snake\n",
    "        agent['velocity'] = 1            \n",
    "        # Returning the Agent Properties\n",
    "        return agent\n",
    "\n",
    "    def _init_image_(self, path):\n",
    "        # Loading the image\n",
    "        image = cv2.imread(path)\n",
    "        # Resizing the image\n",
    "        image = cv2.resize(image, (self.scale-1,self.scale-1), interpolation=cv2.INTER_NEAREST)\n",
    "        # Returning the preprocessed image\n",
    "        return image\n",
    "\n",
    "    def __init__(self, n, rewards, game_length=100):\n",
    "        # Parameters\n",
    "        self.n = n\n",
    "        self.rewards = rewards\n",
    "        self.scale = 256//self.n\n",
    "        # Actions we can take : left = 0, forward = 1, right = 2\n",
    "        self.action_space = Discrete(3)\n",
    "        # The nxn grid\n",
    "        self.observation_space = MultiDiscrete([self.n, self.n])\n",
    "        # Set Total Game length\n",
    "        self.game_length = game_length\n",
    "        self.game_length_ = self.game_length\n",
    "        # Set Food Spawning locations. Totally there are only n locations\n",
    "        self.foodspawn_space = self._food_position_(self.n)\n",
    "        # Out of the n food locations, at any time only n/2 random locations have food\n",
    "        self.live_index = np.random.choice(len(self.foodspawn_space), size=(self.n//2), replace=False)\n",
    "        self.live_foodspawn_space = self.foodspawn_space[self.live_index]\n",
    "        # Initializing the Agents\n",
    "        self.agentA = self._init_agent_()\n",
    "        self.agentB = self._init_agent_()\n",
    "        # Loading the Images\n",
    "        self.image_agentA = self._init_image_(\"agentA.png\")\n",
    "        self.image_agentB = self._init_image_(\"agentB.png\")\n",
    "        self.image_prey = self._init_image_(\"prey.png\")\n",
    "        # Creating History\n",
    "        encoded, _ = self.encode()\n",
    "        self.history = [] # {\"agentA\": self.agentA, \"agentB\":self.agentB, \"live_foodspawn_space\": self.live_foodspawn_space, 'encoded': encoded}\n",
    "\n",
    "    def _movement_(self, action, agent):\n",
    "        # Loading the states\n",
    "        illegal = 0     # If the snake hits the walls\n",
    "        n = self.n\n",
    "        head = agent['head']\n",
    "        state = agent['state'].copy()\n",
    "        velocity = agent['velocity']\n",
    "        score = agent['score']\n",
    "        # Applying the Action\n",
    "        if action == 0: # Go Left\n",
    "            if head==0:\n",
    "                if state[1]==velocity-1: # Left Wall\n",
    "                    illegal = 1\n",
    "                    change = np.array([0, 0])\n",
    "                else:\n",
    "                    change = np.array([0, -velocity])\n",
    "                head = 3\n",
    "            elif head==1:\n",
    "                if state[0]==velocity-1: # Top Wall\n",
    "                    illegal = 1\n",
    "                    change = np.array([0, 0])\n",
    "                else:\n",
    "                    change = np.array([-velocity, 0])\n",
    "                head = 0\n",
    "            elif head==2: \n",
    "                if state[1]==n-velocity: # Right Wall\n",
    "                    illegal = 1\n",
    "                    change = np.array([0, 0])\n",
    "                else:\n",
    "                    change = np.array([0, velocity])\n",
    "                head = 1\n",
    "            elif head==3:\n",
    "                if state[0]==n-velocity: # Bottom Wall\n",
    "                    illegal = 1\n",
    "                    change = np.array([0, 0])\n",
    "                else:\n",
    "                    change = np.array([velocity, 0])\n",
    "                head = 2           \n",
    "        elif action == 1: # Move Forward\n",
    "            if head==0:\n",
    "                if state[0]==velocity-1: # Top Wall\n",
    "                    illegal = 1\n",
    "                    change = np.array([0, 0])\n",
    "                else:\n",
    "                    change = np.array([-velocity, 0])\n",
    "                head = 0\n",
    "            elif head==1:\n",
    "                if state[1]==n-velocity: # Right Wall\n",
    "                    illegal = 1\n",
    "                    change = np.array([0, 0])\n",
    "                else:\n",
    "                    change = np.array([0, velocity])\n",
    "                head = 1\n",
    "            elif head==2:\n",
    "                if state[0]==n-velocity: # Bottom Wall\n",
    "                    illegal = 1\n",
    "                    change = np.array([0, 0])\n",
    "                else:\n",
    "                    change = np.array([velocity, 0])\n",
    "                head = 2\n",
    "            elif head==3:\n",
    "                if state[1]==velocity-1: # Left Wall\n",
    "                    illegal = 1\n",
    "                    change = np.array([0, 0])\n",
    "                else:\n",
    "                    change = np.array([0, -velocity])\n",
    "                head = 3\n",
    "        elif action == 2: # Go Right\n",
    "            if head==0:\n",
    "                if state[1]==n-velocity: # Right Wall\n",
    "                    illegal = 1\n",
    "                    change = np.array([0, 0])\n",
    "                else:\n",
    "                    change = np.array([0, velocity])\n",
    "                head = 1\n",
    "            elif head==1:\n",
    "                if state[0]==n-velocity: # Bottom Wall\n",
    "                    illegal = 1\n",
    "                    change = np.array([0, 0])\n",
    "                else:\n",
    "                    change = np.array([velocity, 0])\n",
    "                head = 2\n",
    "            elif head==2:\n",
    "                if state[1]==velocity-1: # Left Wall\n",
    "                    illegal = 1\n",
    "                    change = np.array([0, 0])\n",
    "                else:\n",
    "                    change = np.array([0, -velocity])\n",
    "                head = 3\n",
    "            elif head==3:\n",
    "                if state[0]==velocity-1: # Top Wall\n",
    "                    illegal = 1\n",
    "                    change = np.array([0, 0])\n",
    "                else:\n",
    "                    change = np.array([-velocity, 0])\n",
    "                head = 0\n",
    "        # Updating the agent properties\n",
    "        modified = {'head': head, 'state':state+change, 'score':score, 'velocity':velocity}\n",
    "        return modified, illegal\n",
    "\n",
    "    def _reward_(self, agent, illegal):\n",
    "        # Loading the states\n",
    "        head = agent['head']\n",
    "        state = agent['state'].copy()\n",
    "        velocity = agent['velocity']\n",
    "        score = agent['score']\n",
    "        # Calculating the reward\n",
    "        if illegal == 1: # If the snake hits the wall\n",
    "            reward = self.rewards['Illegal']\n",
    "        else:\n",
    "            if True in np.all((state == self.live_foodspawn_space), axis = 1):\n",
    "                # Finding the index of the state\n",
    "                index = np.where(np.all((state == self.live_foodspawn_space), axis = 1) == True)[0].item()\n",
    "                # Computing the empty foodspawn spaces\n",
    "                empty_foodspawn_space = [space for space in self.foodspawn_space if space not in self.live_foodspawn_space]\n",
    "                # Removing the state from live foodspawn space\n",
    "                self.live_foodspawn_space = np.delete(self.live_foodspawn_space, index, 0)\n",
    "                # Updating the live foodspawn space\n",
    "                addition = np.random.choice(len(empty_foodspawn_space), size=1, replace=False)\n",
    "                self.live_foodspawn_space = np.append(self.live_foodspawn_space, np.expand_dims(empty_foodspawn_space[addition.item(0)], axis = 0), axis=0)\n",
    "                assert  len(set([(x,y) for (x,y) in self.live_foodspawn_space])) == 4\n",
    "                # If the snake lands on the food\n",
    "                reward = self.rewards['Food']\n",
    "            else:\n",
    "                # If the snake just moves\n",
    "                reward = self.rewards['Movement']\n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        actionA = action['actionA']\n",
    "        actionB = action['actionB']\n",
    "        # Applying the actions\n",
    "        self.agentA, illegalA = self._movement_(actionA, self.agentA)\n",
    "        self.agentB, illegalB = self._movement_(actionB, self.agentB) \n",
    "        # Calculating the reward\n",
    "        if (self.agentA['state'] == self.agentB['state']).all():\n",
    "            if self.agentA['score'] > self.agentB['score']:\n",
    "                rewardA = 5 * abs( self.agentB['score']//(self.agentA['score']-self.agentB['score']) )\n",
    "                rewardB = - 3 * abs( self.agentB['score']//(self.agentA['score']-self.agentB['score']) )\n",
    "                _ = self._reward_(self.agentA, illegalA)\n",
    "                score = self.agentB['score']\n",
    "                while True:\n",
    "                    self.agentB = self._init_agent_(score)\n",
    "                    if (self.agentB['state']!=self.agentA['state']).all():\n",
    "                        _ = self._reward_(self.agentB, illegalB)\n",
    "                        break\n",
    "            elif self.agentA['score'] < self.agentB['score']:\n",
    "                rewardA = - 3 * abs( self.agentA['score']//(self.agentA['score']-self.agentB['score']) )\n",
    "                rewardB = 5 * abs( self.agentA['score']//(self.agentA['score']-self.agentB['score']) )\n",
    "                _ = self._reward_(self.agentB, illegalB)\n",
    "                score = self.agentA['score']\n",
    "                while True:\n",
    "                    self.agentA = self._init_agent_(score) \n",
    "                    if (self.agentA['state']!=self.agentB['state']).all():\n",
    "                        _ = self._reward_(self.agentA, illegalA)\n",
    "                        break\n",
    "            elif self.agentA['score'] == self.agentB['score']:\n",
    "                rewardA = - abs(self.agentA['score']//2)\n",
    "                rewardB = - abs(self.agentB['score']//2)\n",
    "                while True:\n",
    "                    self.agentA = self._init_agent_(score=self.agentA['score'])\n",
    "                    if (self.agentA['state']!=self.agentB['state']).all():\n",
    "                        _ = self._reward_(self.agentA, illegalA)\n",
    "                        break\n",
    "                while True:\n",
    "                    self.agentB = self._init_agent_(score=self.agentB['score'])\n",
    "                    if (self.agentB['state']!=self.agentA['state']).all():\n",
    "                        _ = self._reward_(self.agentB, illegalB)\n",
    "                        break\n",
    "        else:\n",
    "            rewardA = self._reward_(self.agentA, illegalA)\n",
    "            rewardB = self._reward_(self.agentB, illegalB)\n",
    "        # Adding the reward to the score\n",
    "        self.agentA['score'] = self.agentA['score'] + rewardA\n",
    "        self.agentB['score'] = self.agentB['score'] + rewardB\n",
    "        # Updating history\n",
    "        encoded, _ = self.encode()\n",
    "        self.history.append({\"agentA\": self.agentA, \"agentB\":self.agentB, \"live_foodspawn_space\": self.live_foodspawn_space, \"encoded\": encoded, \n",
    "                             \"rewardA\": rewardA, \"actionA\": actionA, \"rewardB\": rewardB, \"actionB\": actionB})\n",
    "        # Check if game is done\n",
    "        self.game_length -= 1\n",
    "        if self.game_length <= 0:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        # Set placeholder for info\n",
    "        info = {'agentA': self.agentA, 'agentB': self.agentB}\n",
    "        return  rewardA, rewardB, done, info\n",
    "\n",
    "    def _rotate_(self, image, direction):\n",
    "        # Rotating the image to rectify the direction of the head\n",
    "        if direction == 1:\n",
    "            image = np.rot90(image.copy(), k = 3)\n",
    "        elif direction == 2: \n",
    "            image = np.rot90(image.copy(), k = 2)\n",
    "        elif direction == 3:\n",
    "            image = np.rot90(image.copy())\n",
    "        return image\n",
    "\n",
    "    def render(self, actionA, actionB): # Returns a one-hot encoded state\n",
    "        # Loading the states\n",
    "        live_foodspawn_space_ = self.history[-2][\"live_foodspawn_space\"]\n",
    "        agentA = self.history[-2][\"agentA\"]\n",
    "        agentB = self.history[-2][\"agentB\"]\n",
    "        snakeA = agentA['state']\n",
    "        snakeB = agentB['state']\n",
    "        # Initializing the state\n",
    "        state = np.ones((self.scale*self.n, 2*self.scale*self.n, 3))*255\n",
    "        # Adding grid lines\n",
    "        for x in range(self.n+1):\n",
    "            state[self.scale*x:self.scale*x+1, :self.scale*self.n] = [0, 0, 0]\n",
    "        for y in range(self.n+1):\n",
    "            state[:, self.scale*y:self.scale*y+1] = [0, 0, 0]\n",
    "        # Adding the live food location\n",
    "        assert  len(set([(x,y) for (x,y) in live_foodspawn_space_])) == 4\n",
    "        for food in live_foodspawn_space_.tolist():\n",
    "            x = int(food[0])\n",
    "            y = int(food[1])\n",
    "            state[self.scale*x+1:self.scale*x+self.scale, self.scale*y+1:self.scale*y+self.scale] = self.image_prey\n",
    "        # Annotating\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        fontScale = 0.4\n",
    "        color = (0, 0, 0)\n",
    "        thickness = 1\n",
    "        direction = {0:\"North\", 1:\"East\", 2:\"South\", 3:\"West\"}\n",
    "        action = {0:\"Left\", 1:\"Forward\", 2:\"Right\", \"None\":\"None\"}\n",
    "        stateA = \"State A: [{0},{1}]\".format(snakeA[0], snakeA[1])\n",
    "        stateB = \"State B: [{0},{1}]\".format(snakeB[0], snakeB[1])\n",
    "        scoreA = \"Score A: \" + str(agentA['score'])\n",
    "        scoreB = \"Score B: \" + str(agentB['score'])\n",
    "        headA = \"Head A: \" + direction[agentA['head']]\n",
    "        headB = \"Head B: \" + direction[agentB['head']]\n",
    "        actionA = \"Action A: \" + action[actionA]\n",
    "        actionB = \"Action B: \" + action[actionB]\n",
    "        # Adding the text\n",
    "        start = 80\n",
    "        state = cv2.putText(state, scoreA, (265, start), font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        state = cv2.putText(state, stateA, (265, start+32), font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        state = cv2.putText(state, headA, (265, start+64), font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        state = cv2.putText(state, actionA, (265, start+96), font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        state = cv2.putText(state, scoreB, (390, start), font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        state = cv2.putText(state, stateB, (390, start+32), font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        state = cv2.putText(state, headB, (390, start+64), font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        state = cv2.putText(state, actionB, (390, start+96), font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        # Adding the agents\n",
    "        image_agentA = self._rotate_(self.image_agentA, agentA['head'])\n",
    "        image_agentB = self._rotate_(self.image_agentB, agentB['head'])\n",
    "        state[self.scale*snakeA[0]+1:self.scale*snakeA[0]+self.scale, self.scale*snakeA[1]+1:self.scale*snakeA[1]+self.scale] = image_agentA\n",
    "        state[self.scale*snakeB[0]+1:self.scale*snakeB[0]+self.scale, self.scale*snakeB[1]+1:self.scale*snakeB[1]+self.scale] = image_agentB\n",
    "        # Returning the state\n",
    "        return state\n",
    "\n",
    "    def encode(self):\n",
    "        # Loading the states\n",
    "        encoder = {'blank': 0, 'foodspawn_space': 1, 'agentA': 2, 'agentB': 3}\n",
    "        state = np.zeros((self.n, self.n))\n",
    "        live_foodspawn_space = self.live_foodspawn_space.astype(np.int)\n",
    "        snakeA = self.agentA['state']\n",
    "        snakeB = self.agentB['state']\n",
    "        # Adding the agents and snakes\n",
    "        state[live_foodspawn_space[:,0], live_foodspawn_space[:,1]] = encoder['foodspawn_space']\n",
    "        state[snakeA[0], snakeA[1]] = encoder['agentA']\n",
    "        state[snakeB[0], snakeB[1]] = encoder['agentB']\n",
    "        # One-Hot encoding the state\n",
    "        encoded = np.eye(len(encoder.keys()))[state.astype(np.int)]\n",
    "        encoded = np.moveaxis(encoded, -1, 0)\n",
    "        # Returning the encoded and state\n",
    "        return encoded, state\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset Total Game length\n",
    "        self.game_length = self.game_length_\n",
    "        # Reset Food Spawning locations\n",
    "        self.foodspawn_space = self._food_position_(self.n)\n",
    "        # Reset Live Food Spawning locations\n",
    "        self.live_index = np.random.choice(len(self.foodspawn_space), size=(self.n//2), replace=False)\n",
    "        self.live_foodspawn_space = self.foodspawn_space[self.live_index]\n",
    "        # Reset Agents\n",
    "        self.agentA = self._init_agent_()\n",
    "        self.agentB = self._init_agent_()\n",
    "        # Clear History\n",
    "        self.history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BTv-xDmNtUAw"
   },
   "outputs": [],
   "source": [
    "config = {'n': 8, 'rewards': {'Food': 4, 'Movement': -1, 'Illegal': -2}, 'game_length': 1000} # You can change during training but not during evaluation\n",
    "\n",
    "env = Vasuki(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_q_table = None\n",
    "SIZE = config['n']\n",
    "FOOD_REWARD = 4\n",
    "epsilon = 0.9\n",
    "EPS_DECAY = 0.9998\n",
    "NUM_EPISODES = 50000\n",
    "NUM_TIMESTEPS = config['game_length']\n",
    "PLAYER_BOT = 'A'\n",
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Q-Table\n"
     ]
    }
   ],
   "source": [
    "# initialize the qtable\n",
    "if start_q_table is None:\n",
    "    print('Creating Q-Table')\n",
    "    q_table = {}\n",
    "    for i in range(-SIZE+1, SIZE):\n",
    "        for j in range(-SIZE+1, SIZE):\n",
    "            for k in range(-SIZE+1, SIZE):\n",
    "                for l in range(-SIZE+1, SIZE):\n",
    "                    for m in range(4):\n",
    "                        q_table[((i, j), (k, l), m)] = [np.random.uniform(-5, 0) for i in range(3)]\n",
    "else:\n",
    "    print('Loading Q-Table')\n",
    "    with open(start_q_table, 'rb') as f:\n",
    "        q_table = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to get input states\n",
    "def get_input_states(env, player='A'):\n",
    "    playerA = env.agentA\n",
    "    playerB = env.agentB\n",
    "    # swap coordinates for another agent\n",
    "    if player != 'A':\n",
    "        print('swapping')\n",
    "        playerA, playerB = playerB, playerA\n",
    "    # getting states for head and foods\n",
    "    foods = env.live_foodspawn_space\n",
    "    head = playerA['head']\n",
    "    # calculating relative coordinates for enemy\n",
    "    enemy_coord = (playerA['state'][0] - playerB['state'][0], playerA['state'][1] - playerB['state'][1])\n",
    "    # calculating relative coordinates for food\n",
    "    nearest_food = None\n",
    "    distance = 100\n",
    "    for food in foods:\n",
    "        temp_distance = np.linalg.norm(playerA['state'] - food)\n",
    "        if temp_distance < distance:\n",
    "            distance = temp_distance\n",
    "            nearest_food = food\n",
    "    food_coord = (playerA['state'][0] - nearest_food[0], playerA['state'][1] - nearest_food[1])\n",
    "    return (food_coord, enemy_coord, head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.0, 5.0), (1, -1), 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example states\n",
    "get_input_states(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Agent using Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "episode_rewards = []\n",
    "action = {}\n",
    "for episode in range(NUM_EPISODES):\n",
    "    episode_reward = 0\n",
    "    # reset the environment for fresh start\n",
    "    env.reset()\n",
    "    for i in range(NUM_TIMESTEPS):\n",
    "        obs = get_input_states(env, PLAYER_BOT)\n",
    "        \n",
    "        # get action\n",
    "        if np.random.random() > epsilon:\n",
    "            # take best action for observation\n",
    "            action['actionA'] = np.argmax(q_table[obs])\n",
    "        else:\n",
    "            # take a random action\n",
    "            action['actionA'] = env.action_space.sample()\n",
    "        \n",
    "        # get random action for agent B\n",
    "        action['actionB'] = env.action_space.sample()\n",
    "        \n",
    "        # take a step\n",
    "        rewardA, rewardB, done, info = env.step(action)\n",
    "        \n",
    "        # let's do the calculation\n",
    "        # getting the new state after an action\n",
    "        new_obs = get_input_states(env, PLAYER_BOT)\n",
    "        max_future_q = np.max(q_table[new_obs])\n",
    "        current_q = q_table[obs][action['actionA']]\n",
    "        \n",
    "        if rewardA >= FOOD_REWARD:\n",
    "            new_q = FOOD_REWARD\n",
    "        else:\n",
    "            # qtable formula\n",
    "            new_q = (1-LEARNING_RATE) * current_q + LEARNING_RATE * (rewardA + DISCOUNT * max_future_q)\n",
    "            \n",
    "        # update the qtable\n",
    "        q_table[obs][action['actionA']] = new_q\n",
    "        \n",
    "        episode_reward += rewardA\n",
    "        \n",
    "    episode_rewards.append(episode_reward)\n",
    "    epsilon *= EPS_DECAY\n",
    "    \n",
    "    if episode % 1000 ==0:\n",
    "        print(f'Episode {episode} : {episode_reward}')\n",
    "        if episode % 20000 == 0:\n",
    "            # saving the qtable\n",
    "            with open(f'qtable-{episode}.pkl', 'wb') as f:\n",
    "                pickle.dump(q_table, f)\n",
    "    \n",
    "print('Avg Episode Rewards:', np.mean(episode_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the qtable\n",
    "with open('best_qtable.pkl', 'wb') as f:\n",
    "    pickle.dump(q_table, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load completely trained qtable - 50000 episodes\n",
    "with open('best_qtable.pkl', 'rb') as f:\n",
    "    q_table = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agentA': {'head': 1, 'state': array([5, 4]), 'score': 367, 'velocity': 1}, 'agentB': {'head': 0, 'state': array([2, 1]), 'score': -1066, 'velocity': 1}}\n",
      "{'agentA': {'head': 1, 'state': array([5, 2]), 'score': 371, 'velocity': 1}, 'agentB': {'head': 0, 'state': array([3, 1]), 'score': -1069, 'velocity': 1}}\n",
      "{'agentA': {'head': 2, 'state': array([3, 4]), 'score': 392, 'velocity': 1}, 'agentB': {'head': 3, 'state': array([5, 0]), 'score': -993, 'velocity': 1}}\n",
      "{'agentA': {'head': 2, 'state': array([1, 0]), 'score': 306, 'velocity': 1}, 'agentB': {'head': 2, 'state': array([5, 2]), 'score': -995, 'velocity': 1}}\n",
      "{'agentA': {'head': 1, 'state': array([1, 1]), 'score': 383, 'velocity': 1}, 'agentB': {'head': 2, 'state': array([1, 5]), 'score': -1012, 'velocity': 1}}\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    done = False\n",
    "    env.reset()\n",
    "    while not done:\n",
    "        obs = get_input_states(env, PLAYER_BOT)\n",
    "        action = {}\n",
    "        action['actionA'] = np.argmax(q_table[obs])\n",
    "        action['actionB'] = env.action_space.sample()\n",
    "        \n",
    "        rewardA, rewardB, done, info = env.step(action)\n",
    "    if done:\n",
    "        print(info) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCkJjkpRIcFC"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gr61vkRO8Inb"
   },
   "source": [
    "# Inferencer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "XWzhi9x28Jpj"
   },
   "outputs": [],
   "source": [
    "class Runner():\n",
    "    def __init__(self, model_A, model_B, checkpoint):\n",
    "        # Path to store the Video\n",
    "        self.checkpoint = checkpoint\n",
    "        # Defining the Environment\n",
    "        config = {'n': 8, 'rewards': {'Food': 4, 'Movement': -1, 'Illegal': -2}, 'game_length': 100} # Should not change for evaluation\n",
    "        self.env = Vasuki(**config)\n",
    "        self.runs = 100\n",
    "        # Trained Policies\n",
    "        self.model_A = model_A # Loaded model with weights\n",
    "        self.model_B = model_B # Loaded model with weights\n",
    "        # Results\n",
    "        self.winner = {'Player_A': 0, 'Player_B': 0}\n",
    "\n",
    "    def reset(self):\n",
    "        self.winner = {'Player_A': 0, 'Player_B': 0}\n",
    "\n",
    "    def evaluate_A(self):\n",
    "        # Uses self.env as the environment and returns the best action for Player A (Blue)\n",
    "        obs = get_input_states(self.env, PLAYER_BOT)\n",
    "        action_A = np.argmax(self.model_A[obs])\n",
    "        return action_A # Action in {0, 1, 2}\n",
    "\n",
    "    def evaluate_B(self):\n",
    "        # Uses self.env as the environment and returns the best action for Player B (Red)\n",
    "        action_B = self.env.action_space.sample()\n",
    "        return action_B # Action in {0, 1, 2}\n",
    "\n",
    "    def visualize(self, run):\n",
    "        self.env.reset()\n",
    "        done = False\n",
    "        video = []\n",
    "        while not done:\n",
    "            # Actions based on the current state using the learned policy \n",
    "            actionA = self.evaluate_A()\n",
    "            actionB = self.evaluate_B()\n",
    "            action = {'actionA': actionA, 'actionB': actionB}\n",
    "            rewardA, rewardB, done, info = self.env.step(action)\n",
    "            # Rendering the enviroment to generate the simulation\n",
    "            if len(self.env.history)>1:\n",
    "                state = self.env.render(actionA, actionB)\n",
    "                encoded, _ = self.env.encode()\n",
    "                state = np.array(state, dtype=np.uint8)\n",
    "                video.append(state)\n",
    "        # Recording the Winner\n",
    "        if self.env.agentA['score'] > self.env.agentB['score']:\n",
    "            self.winner['Player_A'] += 1\n",
    "        elif self.env.agentB['score'] > self.env.agentA['score']:\n",
    "            self.winner['Player_B'] += 1\n",
    "        # Generates a video simulation of the game\n",
    "        if run%100==0:\n",
    "            aviname = os.path.join(self.checkpoint, f\"game_{run}.avi\")\n",
    "            mp4name = os.path.join(self.checkpoint, f\"game_{run}.mp4\")\n",
    "            w, h, _ = video[0].shape\n",
    "            out = cv2.VideoWriter(aviname, cv2.VideoWriter_fourcc(*'DIVX'), 2, (h, w))\n",
    "            for state in video:\n",
    "                assert state.shape==(256,512,3)\n",
    "                out.write(state)\n",
    "            cv2.destroyAllWindows()\n",
    "            os.popen(\"ffmpeg -i {input} {output}\".format(input=aviname, output=mp4name))\n",
    "            # os.popen(\"rm -f {input}\".format(input=aviname))\n",
    "\n",
    "    def arena(self):\n",
    "        # Pitching the Agents against each other\n",
    "        for run in range(1, self.runs+1, 1):\n",
    "            self.visualize(run)\n",
    "        return self.winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Player_A': 100, 'Player_B': 0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contest = Runner(q_table, None, \"\")\n",
    "contest.arena()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "RL_Games.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
